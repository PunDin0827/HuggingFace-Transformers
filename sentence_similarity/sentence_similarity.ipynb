{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Step.1 導入相關套件*"
      ],
      "metadata": {
        "id": "axtee06KxNn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "1MecSjrBxpjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer , AutoModelForSequenceClassification , Trainer , TrainingArguments\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "_RtDTeEaxRb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.2 載入數據*"
      ],
      "metadata": {
        "id": "pi5qkSTlxxgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"json\" , data_files=\"./train_pair_1w.json\" , split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "Q-6JkYbpxjjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = dataset.train_test_split(test_size=0.2)\n",
        "datasets"
      ],
      "metadata": {
        "id": "UZiFyNMKykOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.3 數據前處理*"
      ],
      "metadata": {
        "id": "eoi7pueUyq-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
        "\n",
        "def process_function(examples):\n",
        "    sentences = []\n",
        "    lables = []\n",
        "    for sen1 , sen2 , label in zip(examples[\"sentence1\"] , examples[\"sentence2\"] , examples[\"label\"]):\n",
        "      sentences.append(sen1)\n",
        "      sentences.append(sen2)\n",
        "      lables.append(1 if int(label)==1 else -1)\n",
        "    # input_ids , attention_mask , token_type_ids\n",
        "    tokenized_examples = tokenizer(sentences, max_length=128, truncation=True , padding=\"max_length\")\n",
        "    tokenized_examples = {k: [v[i : i + 2]for i in range(0 ,len(v),2)] for k , v in tokenized_examples.items()}\n",
        "    tokenized_examples[\"labels\"] = lables\n",
        "    return tokenized_examples\n",
        "\n",
        "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "dKebf10KyuL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets[\"train\"][0])"
      ],
      "metadata": {
        "id": "F9h9uxuI2Qk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.4 建立模型*"
      ],
      "metadata": {
        "id": "YC80NCm92VXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification , BertPreTrainedModel , BertModel\n",
        "from typing import Optional\n",
        "from torch.nn import CosineSimilarity , CosineEmbeddingLoss\n",
        "\n",
        "\n",
        "class DualModel(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
        "      super().__init__(config, *inputs, **kwargs)\n",
        "      self.bert = BertModel(config)\n",
        "      self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "\n",
        "        # step.1 分別取得sentenceA和sentenceB的輸入\n",
        "        senA_input_ids , senB_input_ids = input_ids[: , 0] , input_ids[: , 1]\n",
        "        senA_attention_mask , senB_attention_mask = attention_mask[: , 0] , attention_mask[: , 1]\n",
        "        sanAtoken_type_ids , senBtoken_type_ids = token_type_ids[: , 0] , token_type_ids[: , 1]\n",
        "\n",
        "        # step.2 分別取得sentenceA和sentenceB的向量\n",
        "        senA_outputs = self.bert(\n",
        "        senA_input_ids,\n",
        "        attention_mask=senA_attention_mask,\n",
        "        token_type_ids=senA_token_type_ids,\n",
        "        position_ids=position_ids,\n",
        "        ead_mask=head_mask,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "        )\n",
        "        senA_pooled_output = senA_outputs[1]    # [batch, hidden]\n",
        "\n",
        "        senB_outputs = self.bert(\n",
        "        senB_input_ids,\n",
        "        attention_mask=senB_attention_mask,\n",
        "        token_type_ids=senB_token_type_ids,\n",
        "        position_ids=position_ids,\n",
        "        head_mask=head_mask,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "        )\n",
        "        senB_pooled_output = senB_outputs[1]    # [batch, hidden]\n",
        "\n",
        "        # step.3 計算相似度\n",
        "        cos = CosineSimilarity()(senA_pooled_output, senB_pooled_output)    # [batch, ]\n",
        "\n",
        "        # step.4 計算loss\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CosineEmbeddingLoss(0.3)\n",
        "            loss = loss_fct(senA_pooled_output, senB_pooled_output, labels)\n",
        "\n",
        "        output = (cos,)\n",
        "        return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "model = DualModel.from_pretrained(\"hfl/chinese-macbert-base\")\n"
      ],
      "metadata": {
        "id": "aqVDWLLT2XBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.5 評估函數*"
      ],
      "metadata": {
        "id": "qZNO3R5Y2uVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "acc_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")"
      ],
      "metadata": {
        "id": "QXIui_g42vVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metric(eval_predict):\n",
        "    predictions, labels = eval_predict\n",
        "    predictions = [int(p > 0.7) for p in predictions]\n",
        "    labels = [int(l > 0) for l in labels]\n",
        "    # predictions = predictions.argmax(axis=-1)\n",
        "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metirc.compute(predictions=predictions, references=labels)\n",
        "    acc.update(f1)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "NK8XuQw6269t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.6 訓練參數*"
      ],
      "metadata": {
        "id": "E9XP7AoA2yc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = TrainingArguments(output_dir=\"./dual_model\",\n",
        "                per_device_train_batch_size = 32,\n",
        "                per_device_eval_batch_size = 32,\n",
        "                logging_steps =10,\n",
        "                eval_strategy = \"epoch\",\n",
        "                save_strategy = \"epoch\",\n",
        "                save_total_limit = 3,\n",
        "                learning_rate = 2e-5,\n",
        "                weight_decay = 0.01,\n",
        "                metric_for_best_model = \"f1\",\n",
        "                load_best_model_at_end = True)\n",
        "train_args"
      ],
      "metadata": {
        "id": "lfHlX5Xm20_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.7 訓練器*"
      ],
      "metadata": {
        "id": "Kom5ikxO35J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "trainer = Trainer(model=model,\n",
        "                  args=train_args,\n",
        "                  tokenizer = tokenizer,\n",
        "                  train_dataset = tokenized_datasets[\"train\"],\n",
        "                  eval_dataset = tokenized_datasets[\"test\"],\n",
        "                  data_collator = DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "                  compute_metrics = eval_metric)"
      ],
      "metadata": {
        "id": "mVPDUm0r33Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.8 模型訓練*"
      ],
      "metadata": {
        "id": "_sfy3-Gk4G4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "50cuO1Wp4Jt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.9 模型評估*"
      ],
      "metadata": {
        "id": "jDy8t7rj4LQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(tokenized_datasets[\"test\"])"
      ],
      "metadata": {
        "id": "4WDiuLrc4PCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.10 模型預測*"
      ],
      "metadata": {
        "id": "or7HFicM4QyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceSimilarityPipeline:\n",
        "\n",
        "    def __init__(self, model, tokenizer) -> None:\n",
        "        self.model = model.bert\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = model.device\n",
        "\n",
        "    def preprocess(self, senA, senB):\n",
        "        return self.tokenizer([senA, senB], max_length=128, truncation=True, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        return self.model(**inputs)[1]  # [2, 768]\n",
        "\n",
        "    def postprocess(self, logits):\n",
        "        cos = CosineSimilarity()(logits[None, 0, :], logits[None,1, :]).squeeze().cpu().item()\n",
        "        return cos\n",
        "\n",
        "    def __call__(self, senA, senB, return_vector=False):\n",
        "        inputs = self.preprocess(senA, senB)\n",
        "        logits = self.predict(inputs)\n",
        "        result = self.postprocess(logits)\n",
        "        if return_vector:\n",
        "            return result, logits\n",
        "        else:\n",
        "            return result\n",
        "\n"
      ],
      "metadata": {
        "id": "x9e6yz3agQn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = SentenceSimilarityPipeline(model, tokenizer)"
      ],
      "metadata": {
        "id": "IF_iEn9-gjPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"我喜歡北京\", \"明天不行\", return_vector=True)"
      ],
      "metadata": {
        "id": "rvpiz5tbgkUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}