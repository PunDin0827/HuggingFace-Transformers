{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Step.1 導入相關套件*"
      ],
      "metadata": {
        "id": "F5rF-1y3LKoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "yZ99VT3qP8xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQW62LII-qwW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer ,AutoModelForQuestionAnswering , TrainingArguments , Trainer , DefaultDataCollator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.2 載入數據*"
      ],
      "metadata": {
        "id": "kQdO9ogoQbw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"cmrc2018\", cache_dir=\"data\")"
      ],
      "metadata": {
        "id": "ZKRfNgkkQbFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "id": "Vx6DsUvCQ4QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[\"train\"][0]"
      ],
      "metadata": {
        "id": "xQNoBUxmQ64E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.3 數據處理*"
      ],
      "metadata": {
        "id": "sXYHHbQTRBk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "4AZApWVsRA2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_dataset = datasets[\"train\"].select(range(10))"
      ],
      "metadata": {
        "id": "KnQfXqj9R4NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_examples = tokenizer(text = sample_dataset[\"question\"],\n",
        "            text_pair = sample_dataset[\"context\"],\n",
        "            return_offsets_mapping = True,\n",
        "            return_overflowing_tokens = True,\n",
        "            stride = 128,\n",
        "            max_length = 384, truncation=\"only_second\", padding=\"max_length\")\n"
      ],
      "metadata": {
        "id": "hl-if2chSFDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_examples.keys()"
      ],
      "metadata": {
        "id": "yMBs_R21Uq2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_examples[\"overflow_to_sample_mapping\"], len(tokenized_examples[\"overflow_to_sample_mapping\"])"
      ],
      "metadata": {
        "id": "aM68lbf-VBbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sen in tokenizer.batch_decode(tokenized_examples[\"input_ids\"][:3]):\n",
        "    print(sen)"
      ],
      "metadata": {
        "id": "cl57vlkaW63j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_examples[\"offset_mapping\"][:3])"
      ],
      "metadata": {
        "id": "5C3PndBzZdB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_examples[\"offset_mapping\"][0], len(tokenized_examples[\"offset_mapping\"][0]))"
      ],
      "metadata": {
        "id": "c5EQIbMHu1So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")"
      ],
      "metadata": {
        "id": "zHhhWR__u3ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for idx, _ in enumerate(sample_mapping):\n",
        "    answer = sample_dataset[\"answers\"][sample_mapping[idx]]\n",
        "    start_char = answer[\"answer_start\"][0]\n",
        "    end_char = start_char + len(answer[\"text\"][0])\n",
        "    # 定位答案在token中的起始位置和结束位置\n",
        "    # 拿到context的起始和結束位置，從左右兩側向答案靠近\n",
        "\n",
        "    context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
        "    context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
        "\n",
        "    offset = tokenized_examples.get(\"offset_mapping\")[idx]\n",
        "    example_ids = []\n",
        "\n",
        "    # 判斷答案是否在context中\n",
        "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
        "        start_token_pos = 0\n",
        "        end_token_pos = 0\n",
        "    else:\n",
        "        token_id = context_start\n",
        "        while token_id <= context_end and offset[token_id][0] < start_char:\n",
        "            token_id += 1\n",
        "        start_token_pos = token_id\n",
        "        token_id = context_end\n",
        "        while token_id >= context_start and offset[token_id][1] > end_char:\n",
        "            token_id -=1\n",
        "        end_token_pos = token_id\n",
        "        example_ids.append([sample_mapping[idx]])\n",
        "\n",
        "    print(answer, start_char, end_char, context_start, context_end, start_token_pos, end_token_pos)\n",
        "    print(\"token answer decode:\", tokenizer.decode(tokenized_examples[\"input_ids\"][idx][start_token_pos: end_token_pos + 1]))\n"
      ],
      "metadata": {
        "id": "CyYCinIyXLsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_func(examples):\n",
        "    tokenized_examples = tokenizer(text = examples[\"question\"],\n",
        "                text_pair = examples[\"context\"],\n",
        "                return_offsets_mapping = True,\n",
        "                return_overflowing_tokens = True,\n",
        "                stride = 128,\n",
        "                max_length = 384, truncation = \"only_second\", padding = \"max_length\")\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    example_ids = []\n",
        "    for idx, _ in enumerate(sample_mapping):\n",
        "        answer = examples[\"answers\"][sample_mapping[idx]]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "        # 定位答案在token中的起始位置和结束位置\n",
        "        # 拿到context的起始和結束位置，從左右兩側向答案靠近\n",
        "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
        "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
        "        offset = tokenized_examples.get(\"offset_mapping\")[idx]\n",
        "        # 判斷答案是否在context中\n",
        "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
        "            start_token_pos = 0\n",
        "            end_token_pos = 0\n",
        "        else:\n",
        "            token_id = context_start\n",
        "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
        "                token_id += 1\n",
        "            start_token_pos = token_id\n",
        "            token_id = context_end\n",
        "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
        "                token_id -=1\n",
        "            end_token_pos = token_id\n",
        "        start_positions.append(start_token_pos)\n",
        "        end_positions.append(end_token_pos)\n",
        "        example_ids.append(examples[\"id\"][sample_mapping[idx]])\n",
        "        tokenized_examples[\"offset_mapping\"][idx] = [\n",
        "            (o if tokenized_examples.sequence_ids(idx)[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][idx])\n",
        "        ]\n",
        "\n",
        "\n",
        "    tokenized_examples[\"example_ids\"] = example_ids\n",
        "    tokenized_examples[\"start_positions\"] = start_positions\n",
        "    tokenized_examples[\"end_positions\"] = end_positions\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "aJeTJMB7cv5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenied_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "tokenied_datasets"
      ],
      "metadata": {
        "id": "-Sxr5sYadwy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenied_datasets[\"train\"][\"offset_mapping\"][1])"
      ],
      "metadata": {
        "id": "nKjh9vlXvZeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenied_datasets[\"train\"][\"example_ids\"][:10]"
      ],
      "metadata": {
        "id": "7sill9n5vaUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "# example轉換feature\n",
        "example_to_feature = collections.defaultdict(list)\n",
        "for idx, example_id in enumerate(tokenied_datasets[\"train\"][\"example_ids\"][:10]):\n",
        "    example_to_feature[example_id].append(idx)\n",
        "example_to_feature"
      ],
      "metadata": {
        "id": "_XKVKU42vdfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.4 模型輸出*"
      ],
      "metadata": {
        "id": "VOe6f5VUehLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "def get_result(start_logits, end_logits, exmaples, features):\n",
        "\n",
        "    predictions = {}\n",
        "    references = {}\n",
        "\n",
        "    # example轉換feature\n",
        "    example_to_feature = collections.defaultdict(list)\n",
        "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
        "        example_to_feature[example_id].append(idx)\n",
        "\n",
        "    # 最優解\n",
        "    n_best = 20\n",
        "    # 最大答案長度\n",
        "    max_answer_length = 30\n",
        "\n",
        "    for example in exmaples:\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "        for feature_idx in example_to_feature[example_id]:\n",
        "            start_logit = start_logits[feature_idx]\n",
        "            end_logit = end_logits[feature_idx]\n",
        "            offset = features[feature_idx][\"offset_mapping\"]\n",
        "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
        "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if offset[start_index] is None or offset[end_index] is None:\n",
        "                        continue\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "                    answers.append({\n",
        "                        \"text\": context[offset[start_index][0]: offset[end_index][1]],\n",
        "                        \"score\": start_logit[start_index] + end_logit[end_index]\n",
        "                    })\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
        "            predictions[example_id] = best_answer[\"text\"]\n",
        "        else:\n",
        "            predictions[example_id] = \"\"\n",
        "        references[example_id] = example[\"answers\"][\"text\"]\n",
        "\n",
        "    return predictions, references"
      ],
      "metadata": {
        "id": "j4wsHRBCvnN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.5 評估函數*"
      ],
      "metadata": {
        "id": "nhPfM_stevKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cmrc_eval import evaluate_cmrc\n",
        "\n",
        "def metirc(pred):\n",
        "    start_logits, end_logits = pred[0]\n",
        "    if start_logits.shape[0] == len(tokenied_datasets[\"validation\"]):\n",
        "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenied_datasets[\"validation\"])\n",
        "    else:\n",
        "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenied_datasets[\"test\"])\n",
        "    return evaluate_cmrc(p, r)"
      ],
      "metadata": {
        "id": "6Db6p9C3erNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.6 建立模型*"
      ],
      "metadata": {
        "id": "hxMfB-okfAIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\")"
      ],
      "metadata": {
        "id": "ZCR35Hc5v_4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.7 訓練參數*"
      ],
      "metadata": {
        "id": "doWf-3VPwBdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir = \"models_for_qa\",\n",
        "    per_device_train_batch_size = 32,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    eval_strategy = \"steps\",\n",
        "    eval_steps = 200,\n",
        "    save_strategy = \"epoch\",\n",
        "    logging_steps = 50,\n",
        "    num_train_epochs = 1\n",
        ")"
      ],
      "metadata": {
        "id": "Ds8B_pv-wEGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.8 訓練器*"
      ],
      "metadata": {
        "id": "_uHeetikwNQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = args,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = tokenied_datasets[\"train\"],\n",
        "    eval_dataset = tokenied_datasets[\"validation\"],\n",
        "    data_collator = DefaultDataCollator()\n",
        ")"
      ],
      "metadata": {
        "id": "I9LH8dIke-so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.9 模型訓練*"
      ],
      "metadata": {
        "id": "uovwmangfHrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "_Yd-0iuffSTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step.8 模型預測*"
      ],
      "metadata": {
        "id": "BBgawJ9cfOrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)"
      ],
      "metadata": {
        "id": "vFhTHR3yfU7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(question=\"小明在哪裡上班？\", context=\"小明在北京上班。\")"
      ],
      "metadata": {
        "id": "t8b2xn1GfXHK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}